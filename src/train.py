"""
Training Module
===============
EntraÃ®nement et Ã©valuation des modÃ¨les de prÃ©diction de stocks.

Ce module contient :
- La classe StockPredictor pour encapsuler le modÃ¨le
- Les fonctions d'entraÃ®nement avec tracking MLflow
- L'Ã©valuation et les mÃ©triques
"""

import os
import json
import joblib
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Tuple, Optional, Any, List

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score
)


class StockPredictor:
    """
    Classe principale pour la prÃ©diction de stocks.

    Encapsule le modÃ¨le ML, le scaler, et fournit des mÃ©thodes
    pour l'entraÃ®nement, la prÃ©diction et l'Ã©valuation.

    Attributes:
        model_type: Type de modÃ¨le ('random_forest', 'gradient_boosting', 'logistic')
        model: Le modÃ¨le entraÃ®nÃ©
        scaler: StandardScaler pour normaliser les features
        feature_names: Liste des noms de features
        metadata: MÃ©tadonnÃ©es du modÃ¨le
    """

    SUPPORTED_MODELS = {
        'random_forest': RandomForestClassifier,
        'gradient_boosting': GradientBoostingClassifier,
        'logistic': LogisticRegression
    }

    DEFAULT_PARAMS = {
        'random_forest': {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'random_state': 42,
            'n_jobs': -1
        },
        'gradient_boosting': {
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1,
            'random_state': 42
        },
        'logistic': {
            'C': 1.0,
            'max_iter': 1000,
            'random_state': 42
        }
    }

    def __init__(
        self,
        model_type: str = 'random_forest',
        params: Optional[Dict[str, Any]] = None
    ):
        """
        Initialise le prÃ©dicteur.

        Args:
            model_type: Type de modÃ¨le Ã  utiliser
            params: ParamÃ¨tres du modÃ¨le (optionnel)
        """
        if model_type not in self.SUPPORTED_MODELS:
            raise ValueError(
                f"ModÃ¨le non supportÃ©: {model_type}. "
                f"Choix: {list(self.SUPPORTED_MODELS.keys())}"
            )

        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.params = params or self.DEFAULT_PARAMS.get(model_type, {})
        self.metadata = {
            'model_type': model_type,
            'created_at': None,
            'trained_at': None,
            'version': '1.0.0'
        }

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray] = None,
        y_val: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        EntraÃ®ne le modÃ¨le.

        Args:
            X_train: Features d'entraÃ®nement
            y_train: Labels d'entraÃ®nement
            X_val: Features de validation (optionnel)
            y_val: Labels de validation (optionnel)
            feature_names: Noms des features (optionnel)

        Returns:
            Dictionnaire avec les mÃ©triques d'entraÃ®nement
        """
        print(f"\nğŸš€ EntraÃ®nement du modÃ¨le {self.model_type}")
        print(f"   Samples: {len(X_train)}")
        print(f"   Features: {X_train.shape[1]}")

        # Sauvegarder les noms de features
        if feature_names is not None:
            self.feature_names = feature_names

        # Normaliser les donnÃ©es
        X_train_scaled = self.scaler.fit_transform(X_train)

        # CrÃ©er et entraÃ®ner le modÃ¨le
        model_class = self.SUPPORTED_MODELS[self.model_type]
        self.model = model_class(**self.params)
        self.model.fit(X_train_scaled, y_train)

        # MÃ©triques sur train
        y_train_pred = self.model.predict(X_train_scaled)
        metrics = {
            'train_accuracy': accuracy_score(y_train, y_train_pred),
            'train_precision': precision_score(y_train, y_train_pred, zero_division=0),
            'train_recall': recall_score(y_train, y_train_pred, zero_division=0),
            'train_f1': f1_score(y_train, y_train_pred, zero_division=0)
        }

        # MÃ©triques sur validation si disponible
        if X_val is not None and y_val is not None:
            val_metrics = self.evaluate(X_val, y_val, prefix='val')
            metrics.update(val_metrics)

        # Mise Ã  jour des mÃ©tadonnÃ©es
        self.metadata['trained_at'] = datetime.now().isoformat()
        self.metadata['n_samples'] = len(X_train)
        self.metadata['n_features'] = X_train.shape[1]
        self.metadata['params'] = self.params

        print(f"\nğŸ“Š MÃ©triques d'entraÃ®nement:")
        for key, value in metrics.items():
            print(f"   {key}: {value:.4f}")

        return metrics

    def predict(
        self,
        X: np.ndarray,
        return_proba: bool = True
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        PrÃ©dit les classes et probabilitÃ©s.

        Args:
            X: Features pour la prÃ©diction
            return_proba: Retourner aussi les probabilitÃ©s

        Returns:
            Tuple de (predictions, probabilities)

        Raises:
            ValueError: Si le modÃ¨le n'est pas entraÃ®nÃ©
        """
        if self.model is None:
            raise ValueError("Le modÃ¨le n'est pas entraÃ®nÃ©. Appelez train() d'abord.")

        # Normaliser
        X_scaled = self.scaler.transform(X)

        # PrÃ©dire
        predictions = self.model.predict(X_scaled)

        # ProbabilitÃ©s
        if return_proba and hasattr(self.model, 'predict_proba'):
            probabilities = self.model.predict_proba(X_scaled)[:, 1]
        else:
            probabilities = predictions.astype(float)

        return predictions, probabilities

    def predict_single(self, features: Dict[str, float]) -> Dict[str, Any]:
        """
        PrÃ©dit pour un seul Ã©chantillon Ã  partir d'un dictionnaire de features.

        Args:
            features: Dictionnaire {feature_name: value}

        Returns:
            Dictionnaire avec la prÃ©diction et les mÃ©tadonnÃ©es
        """
        # Convertir en array
        X = np.array([[features.get(f, 0) for f in self.feature_names]])

        predictions, probabilities = self.predict(X)

        return {
            'prediction': int(predictions[0]),
            'probability': float(probabilities[0]),
            'direction': 'UP' if predictions[0] == 1 else 'DOWN',
            'confidence': float(max(probabilities[0], 1 - probabilities[0]))
        }

    def evaluate(
        self,
        X: np.ndarray,
        y: np.ndarray,
        prefix: str = 'test'
    ) -> Dict[str, Any]:
        """
        Ã‰value le modÃ¨le sur un ensemble de donnÃ©es.

        Args:
            X: Features
            y: Labels rÃ©els
            prefix: PrÃ©fixe pour les noms de mÃ©triques

        Returns:
            Dictionnaire avec toutes les mÃ©triques
        """
        predictions, probabilities = self.predict(X)

        metrics = {
            f'{prefix}_accuracy': accuracy_score(y, predictions),
            f'{prefix}_precision': precision_score(y, predictions, zero_division=0),
            f'{prefix}_recall': recall_score(y, predictions, zero_division=0),
            f'{prefix}_f1': f1_score(y, predictions, zero_division=0),
        }

        # AUC-ROC
        try:
            metrics[f'{prefix}_auc_roc'] = roc_auc_score(y, probabilities)
        except Exception:
            metrics[f'{prefix}_auc_roc'] = 0.0

        # Matrice de confusion
        metrics['confusion_matrix'] = confusion_matrix(y, predictions).tolist()

        return metrics

    def get_feature_importance(self) -> Optional[pd.DataFrame]:
        """
        Retourne l'importance des features (si disponible).

        Returns:
            DataFrame avec les features et leur importance, ou None
        """
        if self.model is None:
            return None

        if hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
        elif hasattr(self.model, 'coef_'):
            importance = np.abs(self.model.coef_[0])
        else:
            return None

        if not self.feature_names:
            self.feature_names = [f'feature_{i}' for i in range(len(importance))]

        df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        return df

    def save(self, path: str) -> str:
        """
        Sauvegarde le modÃ¨le et ses composants.

        Args:
            path: RÃ©pertoire de sauvegarde

        Returns:
            Chemin du rÃ©pertoire de sauvegarde
        """
        os.makedirs(path, exist_ok=True)

        # Sauvegarder le modÃ¨le
        model_path = os.path.join(path, 'model.joblib')
        joblib.dump(self.model, model_path)

        # Sauvegarder le scaler
        scaler_path = os.path.join(path, 'scaler.joblib')
        joblib.dump(self.scaler, scaler_path)

        # Sauvegarder les mÃ©tadonnÃ©es
        self.metadata['feature_names'] = self.feature_names
        metadata_path = os.path.join(path, 'metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(self.metadata, f, indent=2)

        print(f"ğŸ’¾ ModÃ¨le sauvegardÃ© dans {path}")
        return path

    @classmethod
    def load(cls, path: str) -> 'StockPredictor':
        """
        Charge un modÃ¨le sauvegardÃ©.

        Args:
            path: RÃ©pertoire contenant le modÃ¨le

        Returns:
            Instance de StockPredictor
        """
        # Charger les mÃ©tadonnÃ©es
        metadata_path = os.path.join(path, 'metadata.json')
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        # CrÃ©er l'instance
        predictor = cls(model_type=metadata['model_type'])
        predictor.metadata = metadata
        predictor.feature_names = metadata.get('feature_names', [])

        # Charger le modÃ¨le
        model_path = os.path.join(path, 'model.joblib')
        predictor.model = joblib.load(model_path)

        # Charger le scaler
        scaler_path = os.path.join(path, 'scaler.joblib')
        predictor.scaler = joblib.load(scaler_path)

        print(f"ğŸ“‚ ModÃ¨le chargÃ© depuis {path}")
        return predictor


def train_and_save(
    model_dir: str = "models",
    data_path: str = "data/raw/stock_data.csv",
    model_type: str = "random_forest"
) -> Dict[str, Any]:
    """
    Pipeline complet d'entraÃ®nement.

    Args:
        model_dir: RÃ©pertoire pour sauvegarder le modÃ¨le
        data_path: Chemin vers les donnÃ©es
        model_type: Type de modÃ¨le

    Returns:
        Dictionnaire avec les mÃ©triques finales
    """
    from data_processing import load_or_generate_data, split_data
    from feature_engineering import create_features, prepare_training_data

    print("=" * 60)
    print("ğŸš€ PIPELINE D'ENTRAINEMENT")
    print("=" * 60)

    # 1. Charger les donnÃ©es
    print("\nğŸ“Š Ã‰tape 1: Chargement des donnÃ©es")
    df = load_or_generate_data(data_path, days=500, seed=42)

    # 2. Feature Engineering
    print("\nğŸ”§ Ã‰tape 2: Feature Engineering")
    df_features = create_features(df)

    # 3. PrÃ©parer les donnÃ©es
    print("\nğŸ“‹ Ã‰tape 3: PrÃ©paration des donnÃ©es")
    X, y, feature_names = prepare_training_data(df_features)

    # 4. Split
    print("\nâœ‚ï¸ Ã‰tape 4: Split train/val/test")
    n = len(X)
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)

    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]

    print(f"   Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")

    # 5. EntraÃ®ner
    print("\nğŸ‹ï¸ Ã‰tape 5: EntraÃ®nement")
    predictor = StockPredictor(model_type=model_type)
    train_metrics = predictor.train(
        X_train, y_train,
        X_val, y_val,
        feature_names=feature_names
    )

    # 6. Ã‰valuation finale sur test
    print("\nğŸ“Š Ã‰tape 6: Ã‰valuation sur Test")
    test_metrics = predictor.evaluate(X_test, y_test, prefix='test')

    print(f"\nğŸ¯ MÃ©triques sur Test:")
    for key, value in test_metrics.items():
        if key != 'confusion_matrix':
            print(f"   {key}: {value:.4f}")

    # 7. Feature Importance
    print("\nğŸ“ˆ Ã‰tape 7: Importance des Features")
    importance = predictor.get_feature_importance()
    if importance is not None:
        print(importance.head(10).to_string(index=False))

    # 8. Sauvegarder
    print("\nğŸ’¾ Ã‰tape 8: Sauvegarde")
    predictor.save(model_dir)

    # Combiner les mÃ©triques
    all_metrics = {**train_metrics, **test_metrics}

    print("\n" + "=" * 60)
    print("âœ… ENTRAINEMENT TERMINÃ‰")
    print("=" * 60)

    return all_metrics


# Point d'entrÃ©e principal
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="EntraÃ®ner le modÃ¨le de prÃ©diction de stocks")
    parser.add_argument('--model-dir', default='models', help='RÃ©pertoire de sortie')
    parser.add_argument('--data-path', default='data/raw/stock_data.csv', help='Chemin des donnÃ©es')
    parser.add_argument('--model-type', default='random_forest',
                        choices=['random_forest', 'gradient_boosting', 'logistic'])

    args = parser.parse_args()

    metrics = train_and_save(
        model_dir=args.model_dir,
        data_path=args.data_path,
        model_type=args.model_type
    )
